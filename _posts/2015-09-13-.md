---
layout: post
title: DirectX 12 and Vulkan Thoughts, Redux
date: 2015-09-13 14:35
author: ventspace
comments: true
categories: [Non-technical]
---
Back in March, I wrote a <a href="http://www.gamedev.net/topic/666419-what-are-your-opinions-on-dx12vulkanmantle/#entry5215019">post on GameDev</a> summing up my thoughts on DX12/Vulkan/Mantle. What I did not expect was that this mini-brain-dump during a quiet moment at work would suddenly be picked up across Twitter and eventually hit a variety of gaming sites. I feel it's about time for a follow up, to elaborate on and clarify a few things and then look at where things have wound up.

(I refer to NVIDIA repeatedly here only because that's my personal experience. I believe that AMD probably does mostly the same things. The rest of the GPU manufacturers are in a slightly different situation.)

<strong>Do most AAA games really ship broken?</strong>
Yes. But let's talk about what "broken" means and why this happens. From the IHV perspective, "broken" does not simply mean the game has bugs (although there's plenty of that). Performance is a big deal too, and PC games are by necessity optimized to run well on a wide array of hardware, much of which behaves very differently with respect to performance. This means writing balanced code that does well on everybody's GPUs, rather than running at maximum on any specific GPU. If I have a code change that runs 10% slower on a GTX Titan Z but 5% faster on everything else, guess what? I'm going to slow down the Titan Z. What do I care? You have a Titan Z, you're already above 60 and my job is done. I'm not going to spend my time trying to get you from 80 to 85. Oh and unless I'm a really big name dev, you probably got that brand new 4K monitor before I did so forget checking those specs at all, nevermind QAing them.

NVIDIA, on the other hand, needs to sell Titan Zs for $1550 each. That 10% I just sacrificed to ship a stable game means the world to them, so they'll simply modify the pipeline to apply the code change I skipped when running on a Titan Z. It's safer for them than for me - their testers have Titan Zs and mine don't, and even if they screw up they can just patch the driver again and nobody bats an eyelid. Now let's suppose they're doing a new hypothetical GTX 990 dual GPU. There are easily a dozen reasons that an existing game won't run at peak on this new card, the developers and game not knowing this card exists for starters. But NVIDIA knows that TomsHardware is going to publish benchmarks on whatever date using GTA V, Metro, Tomb Raider, Middle Earth, and Ashes. The solution is a new beta driver that modifies those games from underneath specifically to make sure those games show the most gigantic performance gains when running those games in 4K max settings or whatever it is.

<strong>What about the games that are actually being bug-fixed by drivers?</strong> 
Okay, fine, so performance and hardware is a hassle. But a lot of people asked, why are developers asking the driver people to fix their mistakes? Why do the driver people put up with this? I remember one case where the developers were using a shader generation system at build time, which used the wrong shaders for NVIDIA on their gold master. They came to NVIDIA to fix the problem because once gold's been submitted, it's out of your hands. Still, this is one of the more unusually egregious examples. Most of the time NVIDIA <i>doesn't actually tell developers</i> that they're applying fixes. It worked during dev, it worked when released, so the developers are happy. Heck, NVIDIA will add entirely new features to games from the driver without ever so much as an email to the devs, sometimes breaking things in the process. Adding SSAO and FXAA to games after the fact comes to mind.

And that brings us to one of the reasons this is actually happening: the design of hardware APIs to date has required the driver to do tons of work to run games. Because the driver is gigantic, the surface area for bugs, regressions, and performance issues of all kinds is also gigantic. If the driver didn't automatically detect and fix these things, people would be tearing their hair out trying to figure out why stuff doesn't work properly without any access to diagnostics that could actually tell them what's wrong. OpenGL in particular had <i>no ability whatsoever</i> to report diagnostic information before about 2012 and the messages were decidedly minimal early on. DirectX did have optional diagnostics but they were generated predominantly or entirely by the MS-written layer, not the NVIDIA/AMD written layer, which left them sadly incomplete as well. Instead NV and AMD produced external analysis tools, which in my experience are horribly temperamental and may not always give you precise information about what the cause of a problem is.

The sum total of all that, plus the realities of in-the-trenches game development, means that most of what you see in the game industry today is the result of large scale trial-and-error development. Sure the Frostbite or Unity or Unreal guys get to sit down with the driver people and figure it out, but most everyone else isn't going to get that level of attention. So we just guess and work until everything seems to work out, with no idea how much the driver's quietly doing for us.

<strong>So the new APIs fix all this insanity?</strong>
Uhh... <a href="http://www.extremetech.com/gaming/213202-ashes-dev-dishes-on-dx12-amd-vs-nvidia-and-asynchronous-compute">yes and no</a>. Oxide heavily leveraged a feature that yields great performance on AMD but not so much on some NV chips. But the feature <i>technically works</i> - it's just ridiculously slow. Here we have a particularly drastic example of the trade offs in engineering for different hardware specs, and the multi-way arguments that break out as a result as everyone tries to figure out who to blame. 
